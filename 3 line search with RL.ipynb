{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Line Search with RL\n",
    "### Still consider the problem of minimizing f (x) = x4 and the initial point x0 = 1.\n",
    "#### Question 7 (6 Points). Discretize the state space and the action space (design your discretization that makes sense for solving the problem). Implement Q-learning. What is the “optimal” sequence of step sizes that Q-learning finds when it converges?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.75 # Discount factor \n",
    "alpha = 0.9 # Learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the states\n",
    "states = [('s-4', -4),\n",
    " ('s-3', -3),\n",
    " ('s-2', -2),\n",
    " ('s-1', -1),\n",
    " ('s0', 0),\n",
    " ('s1', 1),\n",
    " ('s2', 2),\n",
    " ('s3', 3),\n",
    " ('s4', 4),\n",
    " 'Terminal']\n",
    "\n",
    "x_to_states = {\n",
    "    -4:0,\n",
    "    -3:1,\n",
    "    -2:2,\n",
    "    -1:3,\n",
    "    0:4,\n",
    "    1:5,\n",
    "    2:6,\n",
    "    3:7,\n",
    "    4:8,\n",
    "}\n",
    "\n",
    "def state_to_fx(x):\n",
    "    return x**4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the actions\n",
    "actions = [1,2,3,4]\n",
    "\n",
    "def action_to_step_size(act):\n",
    "    return 1/(4*act**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('s-1', -1): 0,\n",
       " ('s-2', -2): 0,\n",
       " ('s-3', -3): 0,\n",
       " ('s-4', -4): 0,\n",
       " ('s0', 0): 1,\n",
       " ('s1', 1): 0,\n",
       " ('s2', 2): 0,\n",
       " ('s3', 3): 0,\n",
       " ('s4', 4): 0,\n",
       " 'Terminal': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the rewards\n",
    "rewards = {('s-1', -1): 0,\n",
    " ('s-2', -2): 0,\n",
    " ('s-3', -3): 0,\n",
    " ('s-4', -4): 0,\n",
    " ('s0', 0): 1,\n",
    " ('s1', 1): 0,\n",
    " ('s2', 2): 0,\n",
    " ('s3', 3): 0,\n",
    " ('s4', 4): 0,\n",
    " 'Terminal': 0}\n",
    "\n",
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(('s-4', -4), 1): 0, (('s-4', -4), 2): 0, (('s-4', -4), 3): 0, (('s-4', -4), 4): 0, (('s-3', -3), 1): 0, (('s-3', -3), 2): 0, (('s-3', -3), 3): 0, (('s-3', -3), 4): 0, (('s-2', -2), 1): 0, (('s-2', -2), 2): 0, (('s-2', -2), 3): 0, (('s-2', -2), 4): 0, (('s-1', -1), 1): 0, (('s-1', -1), 2): 0, (('s-1', -1), 3): 0, (('s-1', -1), 4): 0, (('s0', 0), 1): 0, (('s0', 0), 2): 0, (('s0', 0), 3): 0, (('s0', 0), 4): 0, (('s1', 1), 1): 0, (('s1', 1), 2): 0, (('s1', 1), 3): 0, (('s1', 1), 4): 0, (('s2', 2), 1): 0, (('s2', 2), 2): 0, (('s2', 2), 3): 0, (('s2', 2), 4): 0, (('s3', 3), 1): 0, (('s3', 3), 2): 0, (('s3', 3), 3): 0, (('s3', 3), 4): 0, (('s4', 4), 1): 0, (('s4', 4), 2): 0, (('s4', 4), 3): 0, (('s4', 4), 4): 0, ('Terminal', 1): 0, ('Terminal', 2): 0, ('Terminal', 3): 0, ('Terminal', 4): 0}\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "Q = {}\n",
    "for i in range(len(states)):\n",
    "    for a in actions:\n",
    "        Q[(states[i],a)] = 0\n",
    "\n",
    "print(Q)\n",
    "print(len(Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_q_of_state(Q,s):\n",
    "    q = -100000\n",
    "    for a in actions:\n",
    "        if Q[(s,a)] > q:\n",
    "            q = Q[(s,a)]\n",
    "    return q\n",
    "\n",
    "def best_action_of_state(Q,s):\n",
    "    q = -100000\n",
    "    for a in actions:\n",
    "        if Q[(s,a)] > q:\n",
    "            q = Q[(s,a)]\n",
    "            ret_a = a\n",
    "    return ret_a\n",
    "\n",
    "def step(s,a):\n",
    "    if abs(s[1]) == a:\n",
    "        return states[4]\n",
    "    else:\n",
    "        return states[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_step_size(initial_x):\n",
    "\n",
    "    # -----------Q-Learning algorithm-----------\n",
    "    eps = 0.1\n",
    "    # Initializing Q-Values\n",
    "    Q = {}\n",
    "    for i in range(len(states)):\n",
    "        for a in actions:\n",
    "            Q[(states[i],a)] = 0\n",
    "\n",
    "    # Q-Learning process\n",
    "    for i in range(1000):\n",
    "        # Pick up a state randomly\n",
    "        state_id = np.random.randint(0,10) # Python excludes the upper bound\n",
    "        current_state = states[state_id]\n",
    "        # For traversing \n",
    "        \n",
    "        for t in range(3):\n",
    "            if current_state == states[9]:\n",
    "                break\n",
    "            reward = rewards[current_state]\n",
    "            \n",
    "            r = np.random.random()\n",
    "            best_action = best_action_of_state(Q, current_state)\n",
    "            if r < eps:\n",
    "                while True:\n",
    "                    action = np.random.choice(actions)         \n",
    "                    if action != best_action:\n",
    "                        next_action = action\n",
    "                        break\n",
    "            else:\n",
    "                next_action = best_action\n",
    "\n",
    "            next_state = step(current_state,next_action)\n",
    "\n",
    "            next_q = max_q_of_state(Q,next_state)\n",
    "            Q[(current_state,next_action)] = Q[(current_state, next_action)]+alpha*(reward+gamma*next_q-Q[(current_state, next_action)])\n",
    "\n",
    "            #print(np.argmax(Q[next_state,]))\n",
    "            #TD = reward + gamma * Q[next_state, np.argmax(Q[next_state,])] - Q[(current_state,next_action)]\n",
    "            #Q[(current_state,next_action)] += alpha *TD \n",
    "            #current_state = next_state\n",
    "        \n",
    "\n",
    "    print(Q)\n",
    "    s = states[x_to_states[initial_x]]\n",
    "    step_size = action_to_step_size(best_action_of_state(Q,s))\n",
    "\n",
    "    return step_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(('s-4', -4), 1): 0.0, (('s-4', -4), 2): 0.0, (('s-4', -4), 3): 0.0, (('s-4', -4), 4): 0.75, (('s-3', -3), 1): 0.0, (('s-3', -3), 2): 0.0, (('s-3', -3), 3): 0.75, (('s-3', -3), 4): 0.0, (('s-2', -2), 1): 0.0, (('s-2', -2), 2): 0.75, (('s-2', -2), 3): 0.0, (('s-2', -2), 4): 0.0, (('s-1', -1), 1): 0.75, (('s-1', -1), 2): 0.0, (('s-1', -1), 3): 0.0, (('s-1', -1), 4): 0.0, (('s0', 0), 1): 1.0, (('s0', 0), 2): 0.999999999999, (('s0', 0), 3): 0.999999, (('s0', 0), 4): 0.99999999999, (('s1', 1), 1): 0.75, (('s1', 1), 2): 0.0, (('s1', 1), 3): 0.0, (('s1', 1), 4): 0.0, (('s2', 2), 1): 0.0, (('s2', 2), 2): 0.75, (('s2', 2), 3): 0.0, (('s2', 2), 4): 0.0, (('s3', 3), 1): 0.0, (('s3', 3), 2): 0.0, (('s3', 3), 3): 0.75, (('s3', 3), 4): 0.0, (('s4', 4), 1): 0.0, (('s4', 4), 2): 0.0, (('s4', 4), 3): 0.0, (('s4', 4), 4): 0.75, ('Terminal', 1): 0, ('Terminal', 2): 0, ('Terminal', 3): 0, ('Terminal', 4): 0}\n",
      "optimal step size for x0=1: 0.25\n"
     ]
    }
   ],
   "source": [
    "initial_x = 1\n",
    "print(\"optimal step size for x0=\" + str(initial_x) + \": \" + str(get_optimal_step_size(initial_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
